{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pylab\n",
    "import seaborn\n",
    "import scipy.stats as sp\n",
    "import pickle\n",
    "import imageio\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import copy\n",
    "import scipy.stats as ss\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "DIV_LINE_WIDTH = 50\n",
    "print(np.__version__)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BUILDING CLASSROOM STEP 0 - LOAD ALPGMM DATA\n",
    "##### Make sure your data is in the data/ folder\n",
    "exp_idx = 0\n",
    "units = dict()\n",
    "\n",
    "def get_all_runs(logdir, condition=None, minlen=20):\n",
    "    #print(logdir)\n",
    "    \"\"\"\n",
    "    Recursively look through logdir for output files produced by\n",
    "    Assumes that any file \"progress.txt\" is a valid hit. \n",
    "    \"\"\"\n",
    "    global exp_idx\n",
    "    global units\n",
    "    datasets = []\n",
    "    for root, _, files in os.walk(logdir):\n",
    "        #print(root)\n",
    "        if 'progress.txt' in files:  ## progress txt not use with toy envs, all data in env_params.pkl\n",
    "            run_name = root[13:]\n",
    "            exp_name = None\n",
    "            try:\n",
    "                config_path = open(os.path.join(root,'config.json'))\n",
    "                config = json.load(config_path)\n",
    "                if 'exp_name' in config:\n",
    "                    exp_name = config['exp_name']\n",
    "                    \n",
    "            except:\n",
    "                print('No file named config.json')\n",
    "            condition1 = condition or exp_name or 'exp'\n",
    "            condition2 = condition1 + '-' + str(exp_idx)\n",
    "            exp_idx += 1\n",
    "            if condition1 not in units:\n",
    "                units[condition1] = 0\n",
    "            unit = units[condition1]\n",
    "            units[condition1] += 1\n",
    "            \n",
    "            performance = 'AverageTestEpRet' if 'AverageTestEpRet' in exp_data else 'AverageEpRet'\n",
    "            exp_data.insert(len(exp_data.columns),'Unit',unit)\n",
    "            exp_data.insert(len(exp_data.columns),'Condition1',condition1)\n",
    "            exp_data.insert(len(exp_data.columns),'Condition2',condition2)\n",
    "            exp_data.insert(len(exp_data.columns),'evaluation return',exp_data[performance])\n",
    "            data_dict = exp_data.to_dict(\"list\")\n",
    "            data_dict['total timesteps'] = []\n",
    "            for e in data_dict['Epoch']:\n",
    "                data_dict['total timesteps'].append(e * config['steps_per_epoch'])\n",
    "            data_dict['config'] = config\n",
    "            #print(config)\n",
    "                \n",
    "            nb_epochs = len(data_dict['total timesteps'])\n",
    "            print('{} -> {}'.format(run_name, nb_epochs))\n",
    "            if nb_epochs >= minlen:\n",
    "                if 'env_params_save.pkl' in files:\n",
    "                    try:\n",
    "                        env_params_dict = pickle.load( open(os.path.join(root,'env_params_save.pkl'), \"rb\" ) )\n",
    "                    except EOFError:\n",
    "                        print('Corrupted save, ignoring {}'.format(data_dict['config']['seed']))\n",
    "                        continue\n",
    "                    for k,v in env_params_dict.items():\n",
    "                        #print(k)\n",
    "#                         if k==\"env_test_rewards\" or k=='env_params_test' or k=='cegt_means' or k=='cegt_covariances'\\\n",
    "#                            or k=='cegt_episodes' or k=='weights' or k=='cegt_tasks_origin' or k=='cegt_expert_idx'\\\n",
    "#                            or k=='cegt_nb_alpgmm_gaussians' or k=='cegt_initial_expert_means'\\\n",
    "#                            or k=='env_train_len' or k=='cegt_episodes' or k=='selected_expert'\\\n",
    "#                            or k=='cegt_test_vectors' or k=='cegt_k' or k=='cegt_pt' or k=='episodes'\\\n",
    "#                            or k=='env_train_rewards':\n",
    "                        if True :# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#                         if k==\"env_test_rewards\" or k=='env_params_test' or k=='episodes'\\\n",
    "#                            or k==\"evaluation return\" or k==\"total timesteps\":\n",
    "                               data_dict[k] = v\n",
    "                        else:\n",
    "                            data_dict[k] = 4242\n",
    "                #print(data_dict.keys())\n",
    "                datasets.append(data_dict)\n",
    "    return datasets\n",
    "#ICLR_data/ignore default final no adapt/\n",
    "def get_datasets(rootdir):#NeurIPS_2020/20_03_classroom_campain/\"):#teachDRL/data/ICLR_data/big_table_data/\n",
    "    _, models_list, _ = next(os.walk(rootdir))\n",
    "    print(models_list)\n",
    "    for dir_name in models_list.copy():\n",
    "        if \"ignore\" in dir_name or \"elders\" in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "    for expe_name in list(labels.keys()):\n",
    "        if expe_name not in models_list:\n",
    "            del labels[expe_name]\n",
    "    \n",
    "#     # setting specific colors for each expe\n",
    "#     for i,m_name in enumerate(models_list):\n",
    "#         if m_name in specific_colors:\n",
    "#             colors[m_name] = specific_colors[m_name]\n",
    "#         else:\n",
    "#             colors[m_name] = default_colors[i]\n",
    "            \n",
    "    # setting per-model type colors    \n",
    "    for i,m_name in enumerate(models_list):\n",
    "        for m_type, m_color in per_model_colors.items():\n",
    "            if m_type in m_name:\n",
    "                colors[m_name] = m_color\n",
    "        print(\"extracting data for {}...\".format(m_name))\n",
    "        m_id = m_name\n",
    "        models_saves[m_id] = OrderedDict()\n",
    "        models_saves[m_id]['data'] = get_all_runs(rootdir+m_name)\n",
    "        print(\"done\")\n",
    "        if m_name not in labels:\n",
    "            labels[m_name] = m_name\n",
    "\n",
    "    \"\"\"\n",
    "    retrieve all experiences located in \"data to vizu\" folder\n",
    "    \"\"\"\n",
    "default_colors = [\"violet\",\"yellow\",'green','black',u'#ff7f0e',\n",
    "                      \"cyan\", \"pink\", u'#1f77b4',\"grey\",\"r\",\n",
    "                     \"darkorchid\",\"sienna\",\"lightpink\",\"blue\",\"magenta\", \"indigo\",\"mediumseagreen\",'aqua',\n",
    "                'deeppink','silver','khaki','goldenrod','y','y','y','y','y','y','y','y','y','y','y','y' ]  + ['y']*50\n",
    "print(len(default_colors))\n",
    "labels = OrderedDict()\n",
    "per_model_colors = OrderedDict()\n",
    "\n",
    "# LOAD DATA\n",
    "models_saves = OrderedDict()\n",
    "colors = OrderedDict()\n",
    "get_datasets(\"teachDRL/data/\")\n",
    "print(per_model_colors)\n",
    "print(labels)\n",
    "if per_model_colors:  # order runs for legend order as in per_models_colors, with corresponding colors\n",
    "    ordered_labels = OrderedDict()\n",
    "    for teacher_type in per_model_colors.keys():\n",
    "        for k,v in labels.items():\n",
    "            if teacher_type in k:\n",
    "                ordered_labels[k] = v\n",
    "    labels = ordered_labels\n",
    "else:\n",
    "    print('not using per_model_color')\n",
    "    for k in models_saves.keys():\n",
    "        labels[k] = k\n",
    "        \n",
    "print(labels)\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## BUILD CLASSROOM #############################################\n",
    "\n",
    "# Construct history of trained students\n",
    "def load_expert_trajectory(data, alp_thr=0.2):\n",
    "    # 1 - loading the trajectory\n",
    "    for k, v in data.items():\n",
    "        if k == 'means' or k == 'covariances' or k == 'weights' or k == 'env_train_len' or k == 'episodes' \\\n",
    "           or k == 'env_train_rewards':\n",
    "            data[k] = v\n",
    "        if k == 'egt_means' or k == 'egt_covariances' or k == 'egt_weights' or k == 'egt_env_train_len' or k == 'egt_episodes' \\\n",
    "                or k == 'egt_env_train_rewards':\n",
    "            data[k[4:]] = v\n",
    "\n",
    "    # 2 - pre-processing expert trajectory\n",
    "    # removing low-alp gaussians\n",
    "    processed_gmms_means = []\n",
    "    processed_gmms_covs = []\n",
    "    processed_gmms_mean_rew = []\n",
    "    idx_removed_gmms = []\n",
    "    #print(data['episodes'])\n",
    "    gmm_step = data['episodes'][0]\n",
    "    max_lp = 0.0\n",
    "    for i, (gmm_means, gmm_covs, episode) in enumerate(zip(data[\"means\"], data[\"covariances\"], data['episodes'])):\n",
    "        #step_nb += sum(data['env_train_len'][i*gmm_step:(i+1)*gmm_step])\n",
    "        processed_gmm_means = []\n",
    "        processed_gmm_covs = []\n",
    "        all_rewards = data['env_train_rewards'][episode:episode + gmm_step] # from gmm\n",
    "        rewards = all_rewards[-50:]  # consider mean reward after some training on the GMM\n",
    "        mean_reward = np.mean(rewards)\n",
    "        for j, (means, covs) in enumerate(zip(gmm_means, gmm_covs)):\n",
    "            if means[-1] > max_lp:\n",
    "                max_lp = means[-1]\n",
    "            if means[-1] > alp_thr:  # last mean is ALP dimension\n",
    "                # add gaussian\n",
    "                processed_gmm_means.append(means)\n",
    "                processed_gmm_covs.append(covs)\n",
    "        if not processed_gmm_means == []:  # gmm not empty after pre-process, lets add it\n",
    "            processed_gmms_means.append(processed_gmm_means)\n",
    "            processed_gmms_covs.append(processed_gmm_covs)\n",
    "            processed_gmms_mean_rew.append(mean_reward)\n",
    "        else:\n",
    "            idx_removed_gmms.append(i)\n",
    "#     print('idx of removed gmms ({}/{}) in expert traj: {}, max_lp={}'.format(len(data['means']) - len(processed_gmms_means),\n",
    "#                                                                   len(data['means']),\n",
    "#                                                                   idx_removed_gmms, max_lp))\n",
    "    #print('number of steps: {}'.format(step_nb))\n",
    "    return len(processed_gmms_means)\n",
    "\n",
    "\n",
    "\n",
    "folder = 'teachDRL/graphics/elders/raw/'\n",
    "\n",
    "import matplotlib.colorbar as cbar\n",
    "import teachDRL.teachers.utils.plot_utils as plotter\n",
    "import pickle\n",
    "\n",
    "ignore_failed_trainings = True  # SET TO TRUE WHEN BUILDING CLASSROOM\n",
    "keep_no_progress = True\n",
    "student_ids = []\n",
    "#student_params = {'leg_s':[], 'is_quad':[]}\n",
    "student_params = {'agent_type':[], 'leg_s':[], 'nn':[]}\n",
    "initial_test_vectors_list = [[],[],[],[],[]] # lists of first test vectors from idx 0 to 4 (ie. 0.5M to 2.5M)\n",
    "last_test_vector = []\n",
    "last_perfs = []\n",
    "max_lps = []\n",
    "\n",
    "\n",
    "\n",
    "classroom_m_ids = ['PUT CLASSROOM NAME HERE']\n",
    "                \n",
    "nb_no_progress = 0\n",
    "print(labels.keys())\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    if not(m_id in classroom_m_ids):\n",
    "        continue\n",
    "    print(m_id)\n",
    "    runs_data = models_saves[m_id]['data']\n",
    "    all_alpgmm_tasks_percentage = []\n",
    "    nb_zero_perf = 0\n",
    "#    nb_zero_perf_small_net = 0\n",
    "#    nb_small_net = 0\n",
    "    for r,run in enumerate(runs_data):\n",
    "        assert(len(run['evaluation return']) == 20)\n",
    "        test_env_vectors = []\n",
    "        last_perf = round(run[\"custom_nb_mastered\"][-1],1) if 'custom_nb_mastered' in run.keys() else round(run[\"nb_mastered\"][-1],1)\n",
    "        max_perf = round(max(run[\"nb_mastered\"]),1)\n",
    "        perf_str = \"last:{}, max:{}\".format(last_perf, max_perf)\n",
    "        if last_perf == 0.0:\n",
    "            nb_zero_perf += 1\n",
    "            max_lps.append(load_expert_trajectory(run, alp_thr=0.2))\n",
    "        perf_title = \"Last perf: {}% | Max perf: {}%\".format(last_perf, max_perf)\n",
    "\n",
    "        tests_rewards = np.array_split(run['env_test_rewards'],20)\n",
    "        initial_score = np.round(sum([np.interp(r, (-150, 350), (0, 1)) for r in tests_rewards[0]]),2)\n",
    "        last_score = np.round(sum([np.interp(r, (-150, 350), (0, 1)) for r in tests_rewards[-1]]),2)\n",
    "\n",
    "        b_type = 0 if 'biped' in run['config']['env_init']['agent_body_type'] else 1\n",
    "        leg_s = run['config']['env_init']['leg_s']\n",
    "        nn = run['config']['ac_kwargs']['hidden_sizes'][0]\n",
    "        print('seed: {}, leg_s:{},b_type:{}, last_perf: {} score={}-->{}: imp:{}'.format(run['config']['seed'],\n",
    "                                                               leg_s, b_type,\n",
    "                                                                   last_perf,\n",
    "                                                                   initial_score,last_score,\n",
    "                                                                    last_score >= initial_score))\n",
    "        \n",
    "        test_params = np.array_split(run['env_params_test'],20)[0]\n",
    "        len_expert_traj = load_expert_trajectory(run, alp_thr=0.2)\n",
    "        if ((last_score >= initial_score or keep_no_progress) and len_expert_traj > 0) or not ignore_failed_trainings: # use in history only if the student managed to progress and has high-lp gaussians\n",
    "            last_perfs.append(last_perf)\n",
    "            student_ids.append(m_id + '_s' + str(run['config']['seed']))\n",
    "            for idx,rews in enumerate(tests_rewards):\n",
    "                test_env_vectors.append(np.array([np.interp(r, (-150, 350), (0, 1)) for r in rews]))\n",
    "                if idx <= 4:\n",
    "                    initial_test_vectors_list[idx].append(np.array([np.interp(r, (-150, 350), (0, 1)) for r in rews]))\n",
    "                if idx == len(tests_rewards)-1:\n",
    "                    last_test_vector.append(np.array([np.interp(r, (-150, 350), (0, 1)) for r in rews]))\n",
    "            #student_params['leg_s'].append(leg_s)\n",
    "            student_params['agent_type'].append(b_type)\n",
    "            student_params['leg_s'].append(leg_s)\n",
    "            student_params['nn'].append(nn)\n",
    "        else:\n",
    "            nb_no_progress += 1\n",
    "            if last_score <= initial_score and len_expert_traj == 0:\n",
    "                print('do not use {}: no progress and no high-lp gaussians'.format(m_id))\n",
    "            elif last_score <= initial_score:\n",
    "                print('do not use {}: no progress'.format(m_id))\n",
    "            elif len_expert_traj == 0:\n",
    "                print('do not use {}: no high-lp gaussians'.format(m_id))\n",
    "\n",
    "print('{}/{} failed learning, removed from classroom'.format(nb_no_progress,len(runs_data)))\n",
    "print('{} in classroom'.format(len(initial_test_vectors_list[0])))\n",
    "if ignore_failed_trainings:\n",
    "    pickle.dump([student_ids, initial_test_vectors_list, last_test_vector, last_perfs, student_params],\n",
    "                open( \"teachDRL/data/elders_knowledge/parkour_classroom.pkl\", \"wb\" ))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
