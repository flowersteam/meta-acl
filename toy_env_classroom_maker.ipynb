{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pylab\n",
    "import seaborn\n",
    "import scipy.stats as sp\n",
    "import pickle\n",
    "import imageio\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import copy\n",
    "import scipy.stats as ss\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "DIV_LINE_WIDTH = 50\n",
    "print(np.__version__)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_runs(logdir, condition=None, minlen=20):\n",
    "    #print(logdir)\n",
    "    \"\"\"\n",
    "    Recursively look through logdir for output files produced by\n",
    "    Assumes that any file \"progress.txt\" is a valid hit. \n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for root, _, files in os.walk(logdir):\n",
    "        print(root)\n",
    "        if 'config.json' in files:\n",
    "            run_name = root[13:]\n",
    "            exp_name = None\n",
    "            try:\n",
    "                config_path = open(os.path.join(root,'config.json'))\n",
    "                config = json.load(config_path)\n",
    "                if 'exp_name' in config:\n",
    "                    exp_name = config['exp_name']\n",
    "                    \n",
    "            except:\n",
    "                print('No file named config.json')\n",
    "            \n",
    "            data_dict = {}\n",
    "            data_dict['config'] = config\n",
    "            #print(config)\n",
    "            \n",
    "            #if nb_epochs >= minlen:\n",
    "            if True:\n",
    "                if 'env_params_save.pkl' in files:\n",
    "                    try:\n",
    "                        env_params_dict = pickle.load( open(os.path.join(root,'env_params_save.pkl'), \"rb\" ) )\n",
    "                    except EOFError:\n",
    "                        print('Corrupted save, ignoring {}'.format(data_dict['config']['seed']))\n",
    "                        continue\n",
    "                    for k,v in env_params_dict.items():\n",
    "                        if k == 'tasks_alps':\n",
    "                            data_dict[k] = None\n",
    "                        elif k == 'tasks_origin':\n",
    "                            data_dict[k] = None\n",
    "                        else:\n",
    "                            data_dict[k] = v\n",
    "#                           if k == 'epochs_score' or k == 'epochs_episode_nb':\n",
    "#                               data_dict[k] = v\n",
    "            \n",
    "                        \n",
    "\n",
    "                #print(data_dict.keys())\n",
    "                datasets.append(data_dict)\n",
    "#                 if not \"epochs_score\" in data_dict:\n",
    "#                     print('ERRRRRRRRRRRRRRRROR {}'.format(run_name))\n",
    "#                 else:\n",
    "                print('{} -> {}'.format(run_name, len(data_dict['epochs_score'])))\n",
    "    return datasets\n",
    "\n",
    "def get_datasets(rootdir):\n",
    "    print('loading all experiments located in {}'.format(rootdir))\n",
    "    _, models_list, _ = next(os.walk(rootdir))\n",
    "    print(models_list)\n",
    "    for dir_name in models_list.copy():\n",
    "        if \"ignore\" in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "    for expe_name in list(labels.keys()):\n",
    "        if expe_name not in models_list:\n",
    "            del labels[expe_name]\n",
    "    \n",
    "#     # setting specific colors for each expe\n",
    "#     for i,m_name in enumerate(models_list):\n",
    "#         if m_name in specific_colors:\n",
    "#             colors[m_name] = specific_colors[m_name]\n",
    "#         else:\n",
    "#             colors[m_name] = default_colors[i]\n",
    "            \n",
    "    # setting per-model type colors    \n",
    "    for i,m_name in enumerate(models_list):\n",
    "        for m_type, m_color in per_model_colors.items():\n",
    "            if m_type in m_name:\n",
    "                colors[m_name] = m_color\n",
    "        print(\"extracting data for {}...\".format(m_name))\n",
    "        m_id = m_name\n",
    "        models_saves[m_id] = OrderedDict()\n",
    "        models_saves[m_id]['data'] = get_all_runs(rootdir+m_name)\n",
    "        print(\"done\")\n",
    "        if m_name not in labels:\n",
    "            labels[m_name] = m_name\n",
    "\n",
    "    \"\"\"\n",
    "    retrieve all experiences located in \"data to vizu\" folder\n",
    "    \"\"\"\n",
    "default_colors = [\"violet\",\"yellow\",'green','black',u'#ff7f0e',\n",
    "                      \"cyan\", \"pink\", u'#1f77b4',\"grey\",\"r\",\n",
    "                     \"darkorchid\",\"sienna\",\"lightpink\",\"blue\",\"magenta\", \"indigo\",\"mediumseagreen\",'aqua',\n",
    "                'deeppink','silver','khaki','goldenrod','y','y','y','y','y','y','y','y','y','y','y','y' ]  + ['y']*50\n",
    "print(len(default_colors))\n",
    "labels = OrderedDict()\n",
    "per_model_colors = OrderedDict()\n",
    "\n",
    "# LOAD DATA\n",
    "models_saves = OrderedDict()\n",
    "colors = OrderedDict()\n",
    "\n",
    "get_datasets(\"teachDRL/data/\") #\n",
    "\n",
    "if per_model_colors:  # order runs for legend order as in per_models_colors, with corresponding colors\n",
    "    ordered_labels = OrderedDict()\n",
    "    for teacher_type in per_model_colors.keys():\n",
    "        for k,v in labels.items():\n",
    "            if teacher_type in k:\n",
    "                ordered_labels[k] = v\n",
    "    labels = ordered_labels\n",
    "else:\n",
    "    print('not using per_model_color')\n",
    "    for k in models_saves.keys():\n",
    "        labels[k] = k\n",
    "        \n",
    "print(labels)\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################  BUILD CLASSROOM  #############################################\n",
    "m_id = \"YOUR EXP NAME HERE\"\n",
    "is_v2 = False\n",
    "if 'v2' in m_id:\n",
    "    is_v2 = True\n",
    "\n",
    "# Construct history of trained students\n",
    "def load_expert_trajectory(data, alp_thr=0.2):\n",
    "    # 1 - loading the trajectory\n",
    "    for k, v in data.items():\n",
    "        if k == 'means' or k == 'covariances' or k == 'weights' or k == 'env_train_len' or k == 'episodes' \\\n",
    "           or k == 'env_train_rewards':\n",
    "            data[k] = v\n",
    "        if k == 'egt_means' or k == 'egt_covariances' or k == 'egt_weights' or k == 'egt_env_train_len' or k == 'egt_episodes' \\\n",
    "                or k == 'egt_env_train_rewards':\n",
    "            data[k[4:]] = v\n",
    "\n",
    "    # 2 - pre-processing expert trajectory\n",
    "    # removing low-alp gaussians\n",
    "    processed_gmms_means = []\n",
    "    processed_gmms_covs = []\n",
    "    processed_gmms_mean_rew = []\n",
    "    idx_removed_gmms = []\n",
    "    gmm_step = data['episodes'][0]\n",
    "    max_lp = 0.0\n",
    "    for i, (gmm_means, gmm_covs, episode) in enumerate(zip(data[\"means\"], data[\"covariances\"], data['episodes'])):\n",
    "        #step_nb += sum(data['env_train_len'][i*gmm_step:(i+1)*gmm_step])\n",
    "        processed_gmm_means = []\n",
    "        processed_gmm_covs = []\n",
    "        all_rewards = data['env_train_rewards'][episode:episode + gmm_step] # from gmm\n",
    "        rewards = all_rewards[-50:]  # consider mean reward after some training on the GMM\n",
    "        mean_reward = np.mean(rewards)\n",
    "        for j, (means, covs) in enumerate(zip(gmm_means, gmm_covs)):\n",
    "            if means[-1] > max_lp:\n",
    "                max_lp = means[-1]\n",
    "            if means[-1] > alp_thr:  # last mean is ALP dimension\n",
    "                # add gaussian\n",
    "                processed_gmm_means.append(means)\n",
    "                processed_gmm_covs.append(covs)\n",
    "        if not processed_gmm_means == []:  # gmm not empty after pre-process, lets add it\n",
    "            processed_gmms_means.append(processed_gmm_means)\n",
    "            processed_gmms_covs.append(processed_gmm_covs)\n",
    "            processed_gmms_mean_rew.append(mean_reward)\n",
    "        else:\n",
    "            idx_removed_gmms.append(i)\n",
    "    print('idx of removed gmms ({}/{}) in expert traj: {}, max_lp={}'.format(len(data['means']) - len(processed_gmms_means),\n",
    "                                                                  len(data['means']),\n",
    "                                                                  idx_removed_gmms, max_lp))\n",
    "    #print('number of steps: {}'.format(step_nb))\n",
    "    return len(processed_gmms_means)\n",
    "\n",
    "\n",
    "folder = 'teachDRL/graphics/elders/raw/'\n",
    "\n",
    "ignore_failed_trainings = True # SET TO TRUE WHEN BUILDING CLASSROOM\n",
    "student_ids = []\n",
    "if is_v2:\n",
    "    student_params = {'start_cube_idx':[]}\n",
    "else:\n",
    "    student_params = {'nb_rot':[]}\n",
    "nb_possible_pretrain_epochs = 10\n",
    "initial_test_vectors_list = [[] for _ in range(nb_possible_pretrain_epochs)] # lists of first test vectors from idx 0 to 10\n",
    "last_test_vector = []\n",
    "last_perfs = []\n",
    "max_lps = []\n",
    "\n",
    "\n",
    "print(m_id)\n",
    "runs_data = models_saves[m_id]['data']\n",
    "#print(len(runs_data))\n",
    "all_alpgmm_tasks_percentage = []\n",
    "nb_zero_perf = 0\n",
    "nb_zero_perf_small_net = 0\n",
    "nb_small_net = 0\n",
    "for r,run in enumerate(runs_data):\n",
    "    test_env_vectors = []\n",
    "    last_perf = round(run[\"epochs_score\"][-1],1)\n",
    "    max_perf = round(max(run[\"epochs_score\"]),1)\n",
    "    perf_str = \"last:{}, max:{}\".format(last_perf, max_perf)\n",
    "    stud_param_name = 'start_cube_idx' if is_v2 else 'nb_rot'\n",
    "    stud_p = run['config'][stud_param_name]\n",
    "    if last_perf == 0.0:\n",
    "        nb_zero_perf += 1\n",
    "        #max_lps.append(load_expert_trajectory(run, alp_thr=0.2))\n",
    "        #print(run['config']['ac_kwargs']['hidden_sizes'])\n",
    "    perf_title = \"Last perf: {}% | Max perf: {}%\".format(last_perf, max_perf)\n",
    "    initial_score = run['epochs_score'][1]  # index 0 is comp at 0 episodes\n",
    "    last_score = run['epochs_score'][-1]\n",
    "    \n",
    "    #if not is_quad and leg_s < 0.3:\n",
    "    print('seed: {}, param:{}, last_perf: {} score={}-->{}: imp:{}'.format(run['config']['seed'],\n",
    "                                                               stud_p,\n",
    "                                                               last_perf,\n",
    "                                                               initial_score,last_score,\n",
    "                                                                last_score >= initial_score))\n",
    "\n",
    "    if (last_score >= initial_score and load_expert_trajectory(run, alp_thr=0.2) > 0) or not ignore_failed_trainings: # use in history only if the student managed to progress and has high-lp gaussians\n",
    "        last_perfs.append(last_perf)\n",
    "        student_ids.append(m_id + '_s' + str(run['config']['seed']))\n",
    "        ep_comp_grids = run['epochs_comp_grid'][1:]  # IMPORTANT: ignore first 0ep perf (always 0)\n",
    "        for idx, ep_comp_grid in enumerate(ep_comp_grids):\n",
    "            if idx < nb_possible_pretrain_epochs:\n",
    "                initial_test_vectors_list[idx].append(ep_comp_grid.flatten())\n",
    "            if idx == len(ep_comp_grids)-1:\n",
    "                last_test_vector.append(ep_comp_grid.flatten())\n",
    "        \n",
    "        student_params[stud_param_name].append(stud_p)\n",
    "        #student_params['is_quad'].append(is_quad)\n",
    "    else:\n",
    "        print('do not use {}: no progress'.format(m_id))\n",
    "         \n",
    "            \n",
    "                \n",
    "print('{}/128 failed learning'.format(nb_zero_perf))\n",
    "#print('{}/{} failed learning'.format(nb_zero_perf_small_net,nb_small_net))\n",
    "print(max_lps)\n",
    "print(len(initial_test_vectors_list[0]))\n",
    "if ignore_failed_trainings:\n",
    "    \n",
    "    pickle.dump([student_ids, initial_test_vectors_list, last_test_vector, last_perfs, student_params],\n",
    "                # WAAAAAAAAAAAAAAAAAAAAAAAAAARNING OTHER\n",
    "                open( \"teachDRL/data/elders_knowledge/toy_classroom.pkl\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
